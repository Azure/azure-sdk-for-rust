#![doc = "generated by AutoRust 0.1.0"]
#![allow(non_camel_case_types)]
#![allow(unused_imports)]
use serde::{Deserialize, Serialize};
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkBatchJobCollection {
    pub from: i32,
    pub total: i32,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub sessions: Vec<SparkBatchJob>,
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkBatchJob {
    #[serde(rename = "livyInfo", default, skip_serializing_if = "Option::is_none")]
    pub livy_info: Option<SparkBatchJobState>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(rename = "workspaceName", default, skip_serializing_if = "Option::is_none")]
    pub workspace_name: Option<String>,
    #[serde(rename = "sparkPoolName", default, skip_serializing_if = "Option::is_none")]
    pub spark_pool_name: Option<String>,
    #[serde(rename = "submitterName", default, skip_serializing_if = "Option::is_none")]
    pub submitter_name: Option<String>,
    #[serde(rename = "submitterId", default, skip_serializing_if = "Option::is_none")]
    pub submitter_id: Option<String>,
    #[serde(rename = "artifactId", default, skip_serializing_if = "Option::is_none")]
    pub artifact_id: Option<String>,
    #[serde(rename = "jobType", default, skip_serializing_if = "Option::is_none")]
    pub job_type: Option<spark_batch_job::JobType>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub result: Option<spark_batch_job::Result>,
    #[serde(rename = "schedulerInfo", default, skip_serializing_if = "Option::is_none")]
    pub scheduler_info: Option<SparkScheduler>,
    #[serde(rename = "pluginInfo", default, skip_serializing_if = "Option::is_none")]
    pub plugin_info: Option<SparkServicePlugin>,
    #[serde(rename = "errorInfo", default, skip_serializing_if = "Vec::is_empty")]
    pub error_info: Vec<SparkServiceError>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<serde_json::Value>,
    pub id: i32,
    #[serde(rename = "appId", default, skip_serializing_if = "Option::is_none")]
    pub app_id: Option<String>,
    #[serde(rename = "appInfo", default, skip_serializing_if = "Option::is_none")]
    pub app_info: Option<serde_json::Value>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub state: Option<spark_batch_job::State>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub log: Vec<String>,
}
pub mod spark_batch_job {
    use super::*;
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum JobType {
        SparkBatch,
        SparkSession,
    }
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum Result {
        Uncertain,
        Succeeded,
        Failed,
        Cancelled,
    }
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum State {
        #[serde(rename = "not_started")]
        NotStarted,
        #[serde(rename = "starting")]
        Starting,
        #[serde(rename = "idle")]
        Idle,
        #[serde(rename = "busy")]
        Busy,
        #[serde(rename = "shutting_down")]
        ShuttingDown,
        #[serde(rename = "error")]
        Error,
        #[serde(rename = "dead")]
        Dead,
        #[serde(rename = "killed")]
        Killed,
        #[serde(rename = "success")]
        Success,
        #[serde(rename = "running")]
        Running,
        #[serde(rename = "recovering")]
        Recovering,
    }
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkBatchJobState {
    #[serde(rename = "notStartedAt", default, skip_serializing_if = "Option::is_none")]
    pub not_started_at: Option<String>,
    #[serde(rename = "startingAt", default, skip_serializing_if = "Option::is_none")]
    pub starting_at: Option<String>,
    #[serde(rename = "runningAt", default, skip_serializing_if = "Option::is_none")]
    pub running_at: Option<String>,
    #[serde(rename = "deadAt", default, skip_serializing_if = "Option::is_none")]
    pub dead_at: Option<String>,
    #[serde(rename = "successAt", default, skip_serializing_if = "Option::is_none")]
    pub success_at: Option<String>,
    #[serde(rename = "killedAt", default, skip_serializing_if = "Option::is_none")]
    pub killed_at: Option<String>,
    #[serde(rename = "recoveringAt", default, skip_serializing_if = "Option::is_none")]
    pub recovering_at: Option<String>,
    #[serde(rename = "currentState", default, skip_serializing_if = "Option::is_none")]
    pub current_state: Option<String>,
    #[serde(rename = "jobCreationRequest", default, skip_serializing_if = "Option::is_none")]
    pub job_creation_request: Option<SparkRequest>,
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkScheduler {
    #[serde(rename = "submittedAt", default, skip_serializing_if = "Option::is_none")]
    pub submitted_at: Option<String>,
    #[serde(rename = "scheduledAt", default, skip_serializing_if = "Option::is_none")]
    pub scheduled_at: Option<String>,
    #[serde(rename = "endedAt", default, skip_serializing_if = "Option::is_none")]
    pub ended_at: Option<String>,
    #[serde(rename = "cancellationRequestedAt", default, skip_serializing_if = "Option::is_none")]
    pub cancellation_requested_at: Option<String>,
    #[serde(rename = "currentState", default, skip_serializing_if = "Option::is_none")]
    pub current_state: Option<spark_scheduler::CurrentState>,
}
pub mod spark_scheduler {
    use super::*;
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum CurrentState {
        Queued,
        Scheduled,
        Ended,
    }
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkServicePlugin {
    #[serde(rename = "preparationStartedAt", default, skip_serializing_if = "Option::is_none")]
    pub preparation_started_at: Option<String>,
    #[serde(rename = "resourceAcquisitionStartedAt", default, skip_serializing_if = "Option::is_none")]
    pub resource_acquisition_started_at: Option<String>,
    #[serde(rename = "submissionStartedAt", default, skip_serializing_if = "Option::is_none")]
    pub submission_started_at: Option<String>,
    #[serde(rename = "monitoringStartedAt", default, skip_serializing_if = "Option::is_none")]
    pub monitoring_started_at: Option<String>,
    #[serde(rename = "cleanupStartedAt", default, skip_serializing_if = "Option::is_none")]
    pub cleanup_started_at: Option<String>,
    #[serde(rename = "currentState", default, skip_serializing_if = "Option::is_none")]
    pub current_state: Option<spark_service_plugin::CurrentState>,
}
pub mod spark_service_plugin {
    use super::*;
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum CurrentState {
        Preparation,
        ResourceAcquisition,
        Queued,
        Submission,
        Monitoring,
        Cleanup,
        Ended,
    }
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkServiceError {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub message: Option<String>,
    #[serde(rename = "errorCode", default, skip_serializing_if = "Option::is_none")]
    pub error_code: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub source: Option<spark_service_error::Source>,
}
pub mod spark_service_error {
    use super::*;
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum Source {
        System,
        User,
        Unknown,
        Dependency,
    }
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkRequest {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    #[serde(rename = "className", default, skip_serializing_if = "Option::is_none")]
    pub class_name: Option<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub args: Vec<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub jars: Vec<String>,
    #[serde(rename = "pyFiles", default, skip_serializing_if = "Vec::is_empty")]
    pub py_files: Vec<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub files: Vec<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub archives: Vec<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conf: Option<serde_json::Value>,
    #[serde(rename = "driverMemory", default, skip_serializing_if = "Option::is_none")]
    pub driver_memory: Option<String>,
    #[serde(rename = "driverCores", default, skip_serializing_if = "Option::is_none")]
    pub driver_cores: Option<i32>,
    #[serde(rename = "executorMemory", default, skip_serializing_if = "Option::is_none")]
    pub executor_memory: Option<String>,
    #[serde(rename = "executorCores", default, skip_serializing_if = "Option::is_none")]
    pub executor_cores: Option<i32>,
    #[serde(rename = "numExecutors", default, skip_serializing_if = "Option::is_none")]
    pub num_executors: Option<i32>,
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkBatchJobOptions {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<serde_json::Value>,
    #[serde(rename = "artifactId", default, skip_serializing_if = "Option::is_none")]
    pub artifact_id: Option<String>,
    pub name: String,
    pub file: String,
    #[serde(rename = "className", default, skip_serializing_if = "Option::is_none")]
    pub class_name: Option<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub args: Vec<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub jars: Vec<String>,
    #[serde(rename = "pyFiles", default, skip_serializing_if = "Vec::is_empty")]
    pub py_files: Vec<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub files: Vec<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub archives: Vec<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conf: Option<serde_json::Value>,
    #[serde(rename = "driverMemory", default, skip_serializing_if = "Option::is_none")]
    pub driver_memory: Option<String>,
    #[serde(rename = "driverCores", default, skip_serializing_if = "Option::is_none")]
    pub driver_cores: Option<i32>,
    #[serde(rename = "executorMemory", default, skip_serializing_if = "Option::is_none")]
    pub executor_memory: Option<String>,
    #[serde(rename = "executorCores", default, skip_serializing_if = "Option::is_none")]
    pub executor_cores: Option<i32>,
    #[serde(rename = "numExecutors", default, skip_serializing_if = "Option::is_none")]
    pub num_executors: Option<i32>,
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkSessionCollection {
    pub from: i32,
    pub total: i32,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub sessions: Vec<SparkSession>,
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkSession {
    #[serde(rename = "livyInfo", default, skip_serializing_if = "Option::is_none")]
    pub livy_info: Option<SparkSessionState>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(rename = "workspaceName", default, skip_serializing_if = "Option::is_none")]
    pub workspace_name: Option<String>,
    #[serde(rename = "sparkPoolName", default, skip_serializing_if = "Option::is_none")]
    pub spark_pool_name: Option<String>,
    #[serde(rename = "submitterName", default, skip_serializing_if = "Option::is_none")]
    pub submitter_name: Option<String>,
    #[serde(rename = "submitterId", default, skip_serializing_if = "Option::is_none")]
    pub submitter_id: Option<String>,
    #[serde(rename = "artifactId", default, skip_serializing_if = "Option::is_none")]
    pub artifact_id: Option<String>,
    #[serde(rename = "jobType", default, skip_serializing_if = "Option::is_none")]
    pub job_type: Option<spark_session::JobType>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub result: Option<spark_session::Result>,
    #[serde(rename = "schedulerInfo", default, skip_serializing_if = "Option::is_none")]
    pub scheduler_info: Option<SparkScheduler>,
    #[serde(rename = "pluginInfo", default, skip_serializing_if = "Option::is_none")]
    pub plugin_info: Option<SparkServicePlugin>,
    #[serde(rename = "errorInfo", default, skip_serializing_if = "Vec::is_empty")]
    pub error_info: Vec<SparkServiceError>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<serde_json::Value>,
    pub id: i32,
    #[serde(rename = "appId", default, skip_serializing_if = "Option::is_none")]
    pub app_id: Option<String>,
    #[serde(rename = "appInfo", default, skip_serializing_if = "Option::is_none")]
    pub app_info: Option<serde_json::Value>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub state: Option<spark_session::State>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub log: Vec<String>,
}
pub mod spark_session {
    use super::*;
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum JobType {
        SparkBatch,
        SparkSession,
    }
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum Result {
        Uncertain,
        Succeeded,
        Failed,
        Cancelled,
    }
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum State {
        #[serde(rename = "not_started")]
        NotStarted,
        #[serde(rename = "starting")]
        Starting,
        #[serde(rename = "idle")]
        Idle,
        #[serde(rename = "busy")]
        Busy,
        #[serde(rename = "shutting_down")]
        ShuttingDown,
        #[serde(rename = "error")]
        Error,
        #[serde(rename = "dead")]
        Dead,
        #[serde(rename = "killed")]
        Killed,
        #[serde(rename = "success")]
        Success,
        #[serde(rename = "running")]
        Running,
        #[serde(rename = "recovering")]
        Recovering,
    }
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkSessionState {
    #[serde(rename = "notStartedAt", default, skip_serializing_if = "Option::is_none")]
    pub not_started_at: Option<String>,
    #[serde(rename = "startingAt", default, skip_serializing_if = "Option::is_none")]
    pub starting_at: Option<String>,
    #[serde(rename = "idleAt", default, skip_serializing_if = "Option::is_none")]
    pub idle_at: Option<String>,
    #[serde(rename = "deadAt", default, skip_serializing_if = "Option::is_none")]
    pub dead_at: Option<String>,
    #[serde(rename = "shuttingDownAt", default, skip_serializing_if = "Option::is_none")]
    pub shutting_down_at: Option<String>,
    #[serde(rename = "killedAt", default, skip_serializing_if = "Option::is_none")]
    pub killed_at: Option<String>,
    #[serde(rename = "recoveringAt", default, skip_serializing_if = "Option::is_none")]
    pub recovering_at: Option<String>,
    #[serde(rename = "busyAt", default, skip_serializing_if = "Option::is_none")]
    pub busy_at: Option<String>,
    #[serde(rename = "errorAt", default, skip_serializing_if = "Option::is_none")]
    pub error_at: Option<String>,
    #[serde(rename = "currentState", default, skip_serializing_if = "Option::is_none")]
    pub current_state: Option<String>,
    #[serde(rename = "jobCreationRequest", default, skip_serializing_if = "Option::is_none")]
    pub job_creation_request: Option<SparkRequest>,
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkSessionOptions {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<serde_json::Value>,
    #[serde(rename = "artifactId", default, skip_serializing_if = "Option::is_none")]
    pub artifact_id: Option<String>,
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    #[serde(rename = "className", default, skip_serializing_if = "Option::is_none")]
    pub class_name: Option<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub args: Vec<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub jars: Vec<String>,
    #[serde(rename = "pyFiles", default, skip_serializing_if = "Vec::is_empty")]
    pub py_files: Vec<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub files: Vec<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub archives: Vec<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conf: Option<serde_json::Value>,
    #[serde(rename = "driverMemory", default, skip_serializing_if = "Option::is_none")]
    pub driver_memory: Option<String>,
    #[serde(rename = "driverCores", default, skip_serializing_if = "Option::is_none")]
    pub driver_cores: Option<i32>,
    #[serde(rename = "executorMemory", default, skip_serializing_if = "Option::is_none")]
    pub executor_memory: Option<String>,
    #[serde(rename = "executorCores", default, skip_serializing_if = "Option::is_none")]
    pub executor_cores: Option<i32>,
    #[serde(rename = "numExecutors", default, skip_serializing_if = "Option::is_none")]
    pub num_executors: Option<i32>,
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkStatementCollection {
    pub total_statements: i32,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub statements: Vec<SparkStatement>,
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkStatement {
    pub id: i32,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub code: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub state: Option<spark_statement::State>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub output: Option<SparkStatementOutput>,
}
pub mod spark_statement {
    use super::*;
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum State {
        #[serde(rename = "waiting")]
        Waiting,
        #[serde(rename = "running")]
        Running,
        #[serde(rename = "available")]
        Available,
        #[serde(rename = "error")]
        Error,
        #[serde(rename = "cancelling")]
        Cancelling,
        #[serde(rename = "cancelled")]
        Cancelled,
    }
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkStatementOutput {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub status: Option<String>,
    pub execution_count: i32,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub data: Option<serde_json::Value>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ename: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub evalue: Option<String>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub traceback: Vec<String>,
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkStatementOptions {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub code: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kind: Option<spark_statement_options::Kind>,
}
pub mod spark_statement_options {
    use super::*;
    #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
    pub enum Kind {
        #[serde(rename = "spark")]
        Spark,
        #[serde(rename = "pyspark")]
        Pyspark,
        #[serde(rename = "dotnetspark")]
        Dotnetspark,
        #[serde(rename = "sql")]
        Sql,
    }
}
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct SparkStatementCancellationResult {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub msg: Option<String>,
}
